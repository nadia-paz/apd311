{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround hitting rows with incomplete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(offset=1, limit = 100_000):\n",
    "    while True:\n",
    "        url = 'https://data.austintexas.gov/resource/xwdj-i9he.json'\n",
    "        \n",
    "        params = {\n",
    "            \"$limit\": limit,\n",
    "            \"$offset\": offset\n",
    "        }\n",
    "\n",
    "        # Make the GET request to the API\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        data_json = response.json()\n",
    "        print(f'start {offset} with {len(data_json)} records')\n",
    "\n",
    "        # if the page has no records, stop iterating\n",
    "        if data_json:\n",
    "            yield data_json\n",
    "            if len(data_json) < limit:\n",
    "                print('Exit from len(data) with offset', offset)\n",
    "                # ti.xcom_push(key='offset', value=offset)\n",
    "                break\n",
    "            else:\n",
    "                offset += limit\n",
    "                #ti.xcom_push(key='offset', value=offset)\n",
    "        else:\n",
    "            # No more data, break the loop\n",
    "            break\n",
    "\n",
    "def preprocess_data(data_json):\n",
    "    \"\"\" \n",
    "    Parameters:\n",
    "        data_json: list of dictionaries from API request\n",
    "    \"\"\"\n",
    "    # create a table\n",
    "    # schema with data types throws errors\n",
    "    # data types changed throgh casting below\n",
    "    if len(data_json[0]) < 20:\n",
    "        raise KeyError\n",
    "    py_table = pa.Table.from_pylist(data_json)\n",
    "    print(py_table.column_names)\n",
    "    # drop not needed columns\n",
    "    # cols_to_drop = [\n",
    "    #         'sr_location',\n",
    "    #         'sr_location_council_district',\n",
    "    #         'sr_location_lat_long', \n",
    "    #         'sr_location_map_tile', \n",
    "    #         'sr_location_map_page',\n",
    "    #         'sr_location_street_number',\n",
    "    #         'sr_location_street_name']\n",
    "    # for col in cols_to_drop:\n",
    "    #     if col in py_table.column_names:\n",
    "    #         py_table = py_table.drop_columns(col)\n",
    "    py_table = py_table.drop_columns([\n",
    "            'sr_location',\n",
    "            # 'sr_location_council_district',\n",
    "            'sr_location_lat_long', \n",
    "            'sr_location_map_tile', \n",
    "            'sr_location_map_page',\n",
    "            # 'sr_location_street_number',\n",
    "            'sr_location_street_name'])\n",
    "    if 'sr_location_street_number' in py_table.column_names:\n",
    "        py_table = py_table.drop_columns('sr_location_street_number')\n",
    "    if 'sr_location_council_district' in py_table.column_names:\n",
    "        py_table = py_table.drop_columns('sr_location_council_district')\n",
    "      \n",
    "    # rename columns, remove sr_ prefix\n",
    "    cols = py_table.column_names\n",
    "    cols = [col[3:] if col!='sr_number' else 'request_id' for col in cols]\n",
    "    py_table = py_table.rename_columns(cols)\n",
    "\n",
    "    # sort columns by type and info\n",
    "    request_info = ['request_id', 'status_desc', 'type_desc', 'method_received_desc']\n",
    "    date_info = ['created_date', 'status_date', 'updated_date']\n",
    "    address_info = ['location_county', 'location_city', 'location_zip_code']\n",
    "    gis_info = ['location_x', 'location_y', 'location_lat', 'location_long']\n",
    "\n",
    "    # cast date \n",
    "    for col in date_info:\n",
    "        arr = py_table[col].cast(pa.timestamp('ms'))\n",
    "        py_table = py_table.drop_columns(col)\n",
    "        py_table = py_table.append_column(col, arr)\n",
    "    \n",
    "    # cast gis info to float\n",
    "    for col in gis_info:\n",
    "        arr = py_table[col].cast(pa.float64())\n",
    "        py_table = py_table.drop_columns(col)\n",
    "        py_table = py_table.append_column(col, arr)\n",
    "    \n",
    "    # new column order\n",
    "    new_order = request_info + date_info + address_info + gis_info\n",
    "    return py_table.select(new_order)\n",
    "\n",
    "def upload_to_gcs(bucket_name, object_name, pq_file):\n",
    "    \"\"\"\n",
    "    Ref: https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python\n",
    "    :param bucket: GCS bucket name\n",
    "    :param object_name: target path & file-name\n",
    "    :param local_file: source path & file-name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # WORKAROUND to prevent timeout for files > 6 MB on 800 kbps upload speed.\n",
    "    # (Ref: https://github.com/googleapis/python-storage/issues/74)\n",
    "    storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024  # 5 MB\n",
    "    # End of Workaround\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    # f\"raw/{pq_file}\"\n",
    "    blob = bucket.blob(object_name)\n",
    "    blob.upload_from_filename(pq_file)\n",
    "\n",
    "def save_data(offset=1, limit = 100_000):\n",
    "    # extract\n",
    "    n = 1\n",
    "    has_data = True\n",
    "    while has_data:\n",
    "        try:\n",
    "            geneartor = get_json(offset, limit)\n",
    "            #for data in geneartor:\n",
    "            data = next(geneartor)\n",
    "            print(offset)\n",
    "            print(\"Data Length:\", len(data))\n",
    "            #data = [*data, *j]\n",
    "            if len(data) == 0:\n",
    "                has_data = False\n",
    "                break\n",
    "            # preprocess\n",
    "            table = preprocess_data(data)\n",
    "            offset = offset + limit\n",
    "            print(\"Data Processed, offset increased by 100 000\")\n",
    "                \n",
    "            # write to local file\n",
    "            filename = f'data_{n:02d}.parquet'\n",
    "            object_name = f\"raw/{filename}\"\n",
    "            local_file = f\"{AIRFLOW_HOME}/{filename}\"\n",
    "            # save table into a file\n",
    "            pq.write_table(table, local_file)\n",
    "            # upload to gcs\n",
    "            upload_to_gcs(GCP_GCS_BUCKET, object_name, local_file)\n",
    "            os.remove(local_file)\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            print(KeyError)\n",
    "            offset = offset + 1\n",
    "            print(\"Offset increased by 1, offset =\", offset)\n",
    "            # continue\n",
    "        except StopIteration:\n",
    "            print(StopIteration)\n",
    "            has_data = False\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 800001 with 100000 records\n",
      "800001\n",
      "Data Length: 100000\n",
      "<class 'KeyError'>\n",
      "Offset increased by 1, offset = 800002\n",
      "start 800002 with 100000 records\n",
      "800002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 900002 with 100000 records\n",
      "900002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1000002 with 100000 records\n",
      "1000002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1100002 with 100000 records\n",
      "1100002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1200002 with 100000 records\n",
      "1200002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1300002 with 100000 records\n",
      "1300002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1400002 with 100000 records\n",
      "1400002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1500002 with 100000 records\n",
      "1500002\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1600002 with 100000 records\n",
      "1600002\n",
      "Data Length: 100000\n",
      "<class 'KeyError'>\n",
      "Offset increased by 1, offset = 1600003\n",
      "start 1600003 with 100000 records\n",
      "1600003\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1700003 with 100000 records\n",
      "1700003\n",
      "Data Length: 100000\n",
      "['sr_location', 'sr_location_council_district', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1800003 with 34085 records\n",
      "1800003\n",
      "Data Length: 34085\n",
      "['sr_location', 'sr_method_received_desc', 'sr_closed_date', 'sr_location_x', 'sr_status_desc', 'sr_type_desc', 'sr_location_county', 'sr_location_map_page', 'sr_location_long', 'sr_updated_date', 'sr_number', 'sr_location_lat', 'sr_location_lat_long', 'sr_status_date', 'sr_location_street_name', 'sr_created_date', 'sr_location_street_number', 'sr_location_city', 'sr_location_y', 'sr_location_map_tile', 'sr_location_zip_code']\n",
      "Data Processed, offset increased by 100 000\n",
      "start 1900003 with 0 records\n",
      "<class 'StopIteration'>\n"
     ]
    }
   ],
   "source": [
    "save_data(offset=800_001, limit=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sr_method_received_desc',\n",
       " 'sr_closed_date',\n",
       " 'sr_status_desc',\n",
       " 'sr_type_desc',\n",
       " 'sr_updated_date',\n",
       " 'sr_number',\n",
       " 'sr_status_date',\n",
       " 'sr_created_date']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://data.austintexas.gov/resource/xwdj-i9he.json'\n",
    "limit = 100\n",
    "offset = 800_001\n",
    "\n",
    "params = {\n",
    "    \"$limit\": limit,\n",
    "    \"$offset\": offset\n",
    "}\n",
    "\n",
    "# Make the GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "data_json = response.json()\n",
    "py_table = pa.Table.from_pylist(data_json)\n",
    "py_table.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
